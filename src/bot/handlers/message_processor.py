"""
Single message processor
"""

import logging
import time
from typing import List, Optional, Dict
from telegram import Update, Message
from telegram.ext import ContextTypes
from telegram.constants import ParseMode

from ...utils.language_context import get_language_context
from ...utils.helpers import (
    format_file_size,
    truncate_text,
    extract_hashtags,
    remove_forward_signature,
    extract_user_comment_from_merged
)

logger = logging.getLogger(__name__)

from ...core.analyzer import ContentAnalyzer
from ...core.storage_manager import StorageManager


async def _process_single_message(message: Message, context: ContextTypes.DEFAULT_TYPE, merged_caption: Optional[str] = None, progress_callback=None) -> tuple:
    """
    å¤„ç†å•æ¡æ¶ˆæ¯ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰
    
    Args:
        message: Telegram message
        context: Bot context
        merged_caption: åˆå¹¶çš„captionæ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•° async def callback(stage: str, progress: float)
        
    Returns:
        (success, result_msg, archive_id, duplicate_info)
        - success: æ˜¯å¦æˆåŠŸ
        - result_msg: ç»“æœæ¶ˆæ¯
        - archive_id: å½’æ¡£IDï¼ˆæˆåŠŸæ—¶ï¼‰
        - duplicate_info: é‡å¤æ–‡ä»¶ä¿¡æ¯ï¼ˆæ£€æµ‹åˆ°é‡å¤æ—¶ï¼‰
    """
    # ä» message åˆ›å»ºä¸´æ—¶ update å¯¹è±¡ä»¥è·å–è¯­è¨€ä¸Šä¸‹æ–‡
    from telegram import Update as TelegramUpdate
    from ...utils.config import get_config
    
    temp_update = TelegramUpdate(update_id=0, message=message)
    lang_ctx = get_language_context(temp_update, context)
    config = get_config()
    
    try:
        # Analyze content
        if progress_callback:
            await progress_callback(lang_ctx.t('progress_analyzing_content'), 0.1)
        
        # å…ˆåšåŸºç¡€åˆ†æ
        analysis = ContentAnalyzer.analyze(message)
        
        # å¦‚æœæ˜¯linkç±»å‹ï¼Œå°è¯•å¼‚æ­¥æå– Telegram é¢„è§ˆ
        if analysis.get('content_type') == 'link':
            try:
                logger.info("Link detected, attempting Telegram preview extraction...")
                analysis = await ContentAnalyzer.analyze_async(message)
                has_preview = analysis.get('telegram_preview') is not None
                logger.info(f"Async analyze completed: has_telegram_preview={has_preview}")
            except Exception as e:
                logger.error(f"Async analyze failed: {e}", exc_info=True)
    
        # æå–æ¶ˆæ¯æ¥æºä¿¡æ¯ï¼ˆæå‰è·å–ç”¨äºæ¸…ç†captionï¼‰
        source_info = None
        is_direct_send = True  # é»˜è®¤æ˜¯ç›´æ¥å‘é€
        
        if message.forward_origin:
            from telegram import MessageOriginChannel, MessageOriginUser, MessageOriginChat, MessageOriginHiddenUser
            
            is_direct_send = False
            if isinstance(message.forward_origin, MessageOriginChannel):
                source_info = {
                    'name': message.forward_origin.chat.title,
                    'id': message.forward_origin.chat.id,
                    'type': message.forward_origin.chat.type
                }
                logger.info(f"Message forwarded from channel: {source_info['name']} (ID: {source_info['id']})")
            elif isinstance(message.forward_origin, MessageOriginChat):
                source_info = {
                    'name': message.forward_origin.sender_chat.title,
                    'id': message.forward_origin.sender_chat.id,
                    'type': message.forward_origin.sender_chat.type
                }
                logger.info(f"Message forwarded from chat: {source_info['name']} (ID: {source_info['id']})")
            elif isinstance(message.forward_origin, MessageOriginUser):
                user = message.forward_origin.sender_user
                source_info = {
                    'name': user.username or user.first_name,
                    'id': user.id,
                    'type': 'bot' if user.is_bot else 'user'
                }
                logger.info(f"Message forwarded from {'bot' if user.is_bot else 'user'}: {source_info['name']} (ID: {source_info['id']})")
            elif isinstance(message.forward_origin, MessageOriginHiddenUser):
                source_info = {
                    'name': message.forward_origin.sender_user_name,
                    'id': None,
                    'type': 'hidden_user'
                }
                logger.info(f"Message forwarded from hidden user: {source_info['name']}")
        else:
            logger.info("Message sent directly by user (not forwarded)")
        
        # æ¸…ç†è½¬å‘æ¶ˆæ¯å°¾éƒ¨ç­¾åï¼ˆæ¥æºå + URLï¼‰
        source_name = source_info.get('name') if source_info else None
        original_caption = analysis.get('content') or message.caption
        cleaned_caption = remove_forward_signature(original_caption, source_name)
        if cleaned_caption != original_caption:
            analysis['content'] = cleaned_caption
            if analysis.get('title') == original_caption:
                analysis['title'] = cleaned_caption or None
        
        # å¦‚æœæœ‰åˆå¹¶çš„captionï¼Œæ·»åŠ åˆ°åˆ†æç»“æœ
        if merged_caption:
            cleaned_merged_caption = remove_forward_signature(merged_caption, source_name)
            # æå–hashtags
            caption_hashtags = extract_hashtags(cleaned_merged_caption or '')
            if caption_hashtags:
                existing_hashtags = analysis.get('hashtags', [])
                analysis['hashtags'] = list(set(existing_hashtags + caption_hashtags))
        
            # ä»…æ·»åŠ ç”¨æˆ·è¯„è®ºï¼Œé¿å…ä¸åŸcaptioné‡å¤
            user_comment = extract_user_comment_from_merged(
                cleaned_merged_caption,
                analysis.get('content') or original_caption
            )
            if user_comment:
                if analysis.get('content'):
                    analysis['content'] = f"{analysis['content']}\n\nğŸ“ {user_comment}"
                else:
                    analysis['content'] = user_comment
        
        # æ–‡ä»¶å»é‡æ£€æµ‹ï¼ˆä»…å¯¹æœ‰æ–‡ä»¶çš„å†…å®¹ï¼‰
        if progress_callback:
            await progress_callback(lang_ctx.t('progress_checking_duplicates'), 0.2)
        
        if analysis.get('file_id'):
            db_storage = context.bot_data.get('db_storage')
            if db_storage:
                duplicate = db_storage.find_duplicate_file(
                    file_id=analysis.get('file_id'),
                    file_name=analysis.get('file_name'),
                    file_size=analysis.get('file_size')
                )
                
                if duplicate:
                    # æ£€æµ‹åˆ°é‡å¤æ–‡ä»¶ï¼Œè¿”å›duplicateä¿¡æ¯ç”±å¤–å±‚ç»Ÿä¸€å¤„ç†
                    logger.info(f"Duplicate file detected: {analysis.get('file_name')}, existing ID: {duplicate['id']}")
                    return False, "æ–‡ä»¶é‡å¤", None, duplicate
        
        # AIæ™ºèƒ½å¤„ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if progress_callback:
            await progress_callback(lang_ctx.t('progress_ai_analysis'), 0.4)
        
        ai_summarizer = context.bot_data.get('ai_summarizer')
        ai_available = ai_summarizer and ai_summarizer.is_available()
        
        if ai_available:
            # 1. ä¼˜å…ˆå¤„ç†ç”µå­ä¹¦åˆ¤æ–­ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if analysis.get('_needs_ai_ebook_check'):
                if progress_callback:
                    await progress_callback(lang_ctx.t('progress_ai_document_type'), 0.35)
                try:
                    file_name = analysis.get('file_name', '')
                    user_language = lang_ctx.language
                    is_ebook = await ai_summarizer.is_ebook(file_name, language=user_language)
                    
                    if is_ebook:
                        analysis['content_type'] = 'ebook'
                        logger.info(f"AIåˆ¤å®šä¸ºç”µå­ä¹¦: {file_name}")
                    else:
                        logger.info(f"AIåˆ¤å®šä¸ºæ™®é€šæ–‡æ¡£: {file_name}")
                        
                except Exception as e:
                    logger.warning(f"AIç”µå­ä¹¦åˆ¤æ–­å¤±è´¥: {e}")
                
                # ç§»é™¤æ ‡è®°
                analysis.pop('_needs_ai_ebook_check', None)
            
            # 2. åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡ŒAIåˆ†æ
            content_type = analysis.get('content_type', '')
            file_size = analysis.get('file_size', 0)
            
            # å¯åˆ†æçš„å†…å®¹ç±»å‹ï¼šæ–‡æœ¬ã€é“¾æ¥ã€æ–‡æ¡£ã€ç”µå­ä¹¦
            analyzable_types = ['text', 'link', 'article', 'document', 'ebook']
            # åª’ä½“ç±»å‹ï¼ˆå›¾ç‰‡ã€è§†é¢‘ç­‰ï¼‰å¦‚æœæœ‰captionæˆ–merged_captionä¹Ÿå¯åˆ†æ
            media_types = ['photo', 'image', 'video', 'audio', 'voice', 'animation']
            # æ–‡æ¡£æ–‡ä»¶æ‰©å±•å
            analyzable_extensions = ['.txt', '.md', '.doc', '.docx', '.pdf', '.epub', '.rtf']
            
            should_analyze = False
            
            # åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ†æ
            if content_type in analyzable_types:
                should_analyze = True
            elif content_type == 'document':
                # æ‰€æœ‰æ”¯æŒæ ¼å¼çš„æ–‡æ¡£éƒ½å¯ä»¥åˆ†æï¼Œä½†å¤§æ–‡ä»¶ä½¿ç”¨å…ƒæ•°æ®æ–¹å¼
                file_name = analysis.get('file_name', '').lower()
                if any(file_name.endswith(ext) for ext in analyzable_extensions):
                    should_analyze = True
            elif content_type in media_types:
                # åª’ä½“ç±»å‹ï¼šå¦‚æœæœ‰captionæˆ–merged_captionåˆ™å¯åˆ†æ
                has_caption = bool(message.caption or merged_caption)
                if has_caption:
                    should_analyze = True
                    logger.info(f"Media {content_type} has caption/comment, will perform AI analysis")
            
            if should_analyze:
                # è‡ªåŠ¨ç”ŸæˆAIæ ‡ç­¾
                if config.ai.get('auto_generate_tags', False):
                    if progress_callback:
                        await progress_callback(lang_ctx.t('progress_ai_generating_tags'), 0.45)
                    try:
                        # ç¡®å®šç”¨äºAIåˆ†æçš„æ–‡æœ¬å†…å®¹
                        # ä¼˜å…ˆçº§ï¼šmerged_captionï¼ˆå«ç”¨æˆ·è¯„è®ºï¼‰ > caption > content
                        content_for_ai = ''
                        if content_type in media_types:
                            # åª’ä½“ç±»å‹ï¼šä¼˜å…ˆä½¿ç”¨merged_captionï¼Œå…¶æ¬¡message.caption
                            content_for_ai = merged_caption or message.caption or ''
                        else:
                            # å…¶ä»–ç±»å‹ï¼šä½¿ç”¨contentæˆ–title
                            content_for_ai = analysis.get('content') or analysis.get('title', '')
                        
                        if content_for_ai:
                            start = time.time()
                            user_language = lang_ctx.language
                            # è·å–é…ç½®çš„æœ€å¤§æ ‡ç­¾æ•°é‡
                            max_tags = config.ai.get('max_generated_tags', 8)
                            max_tags = max(5, min(10, int(max_tags)))  # é™åˆ¶åœ¨5-10ä¹‹é—´
                            
                            ai_tags = await ai_summarizer.generate_tags(content_for_ai, max_tags, language=user_language)
                            duration = time.time() - start
                            provider = getattr(ai_summarizer, '_last_call_info', {}).get('provider', 'unknown')
                            logger.info(f"AI generate_tags provider={provider}, duration={duration:.2f}s, max_tags={max_tags}")
                            
                            if ai_tags:
                                existing_tags = analysis.get('tags', [])
                                
                                # æ™ºèƒ½ç”„åˆ«captionæ ‡ç­¾
                                extract_from_caption = config.get('features.extract_tags_from_caption', False)
                                if not extract_from_caption and message.caption:
                                    # ä»captionä¸­æå–æ½œåœ¨æ ‡ç­¾è¿›è¡Œç”„åˆ«
                                    caption_tags = extract_hashtags(message.caption)
                                    if caption_tags:
                                        # AIç”„åˆ«ï¼šåªä¿ç•™ä¸AIç”Ÿæˆæ ‡ç­¾è¯­ä¹‰ç›¸å…³çš„captionæ ‡ç­¾
                                        filtered_caption_tags = []
                                        for ctag in caption_tags:
                                            if any(ai_tag.lower() in ctag.lower() or ctag.lower() in ai_tag.lower() for ai_tag in ai_tags):
                                                filtered_caption_tags.append(ctag)
                                        
                                        analysis['tags'] = list(set(existing_tags + ai_tags + filtered_caption_tags))
                                        if filtered_caption_tags:
                                            logger.info(f"Filtered caption tags: {filtered_caption_tags} (from {caption_tags})")
                                    else:
                                        analysis['tags'] = list(set(existing_tags + ai_tags))
                                else:
                                    analysis['tags'] = list(set(existing_tags + ai_tags))
                                
                                logger.info(f"AI generated tags: {ai_tags}")
                    except Exception as e:
                        logger.warning(f"AI tag generation failed: {e}")
                
                # è‡ªåŠ¨ç”Ÿæˆæ‘˜è¦
                if config.ai.get('auto_summarize', False):
                    if progress_callback:
                        await progress_callback(lang_ctx.t('progress_ai_analyzing_content'), 0.5)
                    try:
                        # ç¡®å®šç”¨äºAIåˆ†æçš„æ–‡æœ¬å†…å®¹
                        content_for_ai = ''
                        file_name = (analysis.get('file_name') or '').lower()
                        
                        # å¯¹äºç”µå­ä¹¦æˆ–å¤§æ–‡ä»¶ï¼Œä½¿ç”¨å…ƒæ•°æ®è€Œéå†…å®¹
                        if file_name.endswith('.epub') or (content_type == 'document' and file_size and file_size > 1 * 1024 * 1024):
                            # æå–ä¹¦åã€æ–‡ä»¶åç­‰å…ƒæ•°æ®ä½œä¸ºåˆ†æä¾æ®
                            title = analysis.get('title', '') or file_name
                            content_for_ai = f"""è¯·åŸºäºä»¥ä¸‹æ–‡ä»¶ä¿¡æ¯è¿›è¡Œåˆ†æï¼š
æ–‡ä»¶åï¼š{title}
æ–‡ä»¶å¤§å°ï¼š{file_size / 1024 / 1024:.2f}MB

é‡è¦æç¤ºï¼š
1. å¦‚æœä½ ç†Ÿæ‚‰è¿™ä¸ªæ–‡ä»¶/ä¹¦ç±ï¼Œè¯·æä¾›å‡†ç¡®çš„ä»‹ç»å’Œåˆ†ç±»
2. å¦‚æœä¸ç¡®å®šæˆ–ä¸äº†è§£ï¼Œè¯·åœ¨æ‘˜è¦ä¸­æ˜ç¡®è¯´æ˜"æ— æ³•ç¡®å®šå…·ä½“å†…å®¹"ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
3. åŸºäºæ–‡ä»¶åã€æ–‡ä»¶æ‰©å±•åã€è·å¾—çš„ä¿¡æ¯ç­‰æä¾›å¯èƒ½çš„åˆ†ç±»å’Œæ ‡ç­¾
4. æ ‡ç­¾åº”åŒ…å«æ–‡ä»¶å±æ€§ï¼ˆå¦‚ï¼šç”µå­ä¹¦ã€å°è¯´ã€æŠ€æœ¯æ–‡æ¡£ã€æ•™ç¨‹ã€ç”µå½±ã€ç…§ç‰‡ã€è¯ä»¶ç…§ç­‰ï¼‰"""
                            logger.info(f"Using metadata for large file analysis: {title} ({file_size / 1024 / 1024:.2f}MB)")
                        elif content_type in media_types:
                            # åª’ä½“ç±»å‹ï¼šä½¿ç”¨merged_captionæˆ–caption
                            content_for_ai = merged_caption or message.caption or ''
                        else:
                            # å…¶ä»–ç±»å‹ï¼šä½¿ç”¨content
                            content_for_ai = analysis.get('content') or ''
                            # æˆªæ–­å†…å®¹ä»¥èŠ‚çœtokenï¼ˆæœ€å¤š4000å­—ç¬¦ï¼Œçº¦1000ä¸ªtokenï¼‰
                            if len(content_for_ai) > 4000:
                                content_for_ai = content_for_ai[:4000] + "...[å†…å®¹å·²æˆªæ–­]"
                        
                        # æ£€æŸ¥å†…å®¹é•¿åº¦æ˜¯å¦è¾¾åˆ°æ‘˜è¦é˜ˆå€¼
                        min_length = config.ai.get('min_content_length_for_summary', 150)
                        if content_for_ai and len(content_for_ai) >= min_length:
                            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
                            context_info = {
                                'content_type': content_type,
                                'file_size': file_size or 0,
                                'existing_tags': analysis.get('tags', []),
                                'title': analysis.get('title', ''),
                                'file_extension': analysis.get('file_name', '').split('.')[-1] if analysis.get('file_name') else ''
                            }
                            
                            summary_result = None
                            start = time.time()
                            summary_result = await ai_summarizer.summarize_content(
                                    content_for_ai, 
                                    language=user_language,
                                    context=context_info
                                )
                            duration = time.time() - start
                            provider = getattr(ai_summarizer, '_last_call_info', {}).get('provider', 'unknown')
                            logger.info(f"AI summarize_content provider={provider}, duration={duration:.2f}s")
                            
                            # è®°å½•å¤±è´¥è¯¦æƒ…
                            if summary_result and not summary_result.get('success'):
                                error_msg = summary_result.get('error', 'Unknown error')
                                logger.error(f"AI summarize failed: {error_msg}")
                            
                            if summary_result and summary_result.get('success'):
                                # å°†AIåˆ†æç»“æœæ·»åŠ åˆ°analysis
                                analysis['ai_summary'] = summary_result.get('summary', '')
                                analysis['ai_key_points'] = summary_result.get('key_points', [])
                                analysis['ai_category'] = summary_result.get('category', '')
                                
                                # å°†AIå»ºè®®çš„æ ‡ç­¾æ·»åŠ åˆ°æ ‡ç­¾åˆ—è¡¨
                                suggested_tags = summary_result.get('suggested_tags', [])
                                if suggested_tags:
                                    existing_tags = analysis.get('tags', [])
                                    analysis['tags'] = list(set(existing_tags + suggested_tags))
                                
                                logger.info(f"AI analysis complete: summary={analysis['ai_summary'][:50]}..., category={analysis['ai_category']}")
                                if progress_callback:
                                    await progress_callback(lang_ctx.t('progress_ai_analysis_complete'), 0.6)
                    except Exception as e:
                        logger.warning(f"AI summary generation failed: {e}")
        
        # ========== AI é™çº§ç­–ç•¥ï¼šAI ä¸å¯ç”¨æ—¶ä½¿ç”¨åŸºç¡€åˆ†æ ==========
        elif not ai_available and config.ai.get('auto_summarize', False):
            # AI æœªé…ç½®æˆ–ä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§åˆ†æ
            try:
                from ..ai.fallback import AIFallbackAnalyzer
                
                user_language = lang_ctx.language
                fallback_result = None
                
                # æ ¹æ®å†…å®¹ç±»å‹é€‰æ‹©é™çº§ç­–ç•¥
                if analysis.get('file_name'):
                    # æ–‡ä»¶åˆ†æ
                    fallback_result = AIFallbackAnalyzer.analyze_file(
                        file_name=analysis.get('file_name', ''),
                        file_ext=analysis.get('file_name', '').split('.')[-1] if '.' in analysis.get('file_name', '') else '',
                        file_size=analysis.get('file_size', 0),
                        language=user_language
                    )
                elif analysis.get('urls'):
                    # URL åˆ†æ
                    url = analysis['urls'][0]
                    fallback_result = AIFallbackAnalyzer.analyze_url(url, language=user_language)
                elif analysis.get('content'):
                    # æ–‡æœ¬åˆ†æ
                    fallback_result = AIFallbackAnalyzer.analyze_text(
                        content=analysis['content'],
                        content_type=content_type,
                        language=user_language
                    )
                
                if fallback_result and fallback_result.get('success'):
                    # ä½¿ç”¨é™çº§åˆ†æç»“æœ
                    if fallback_result.get('category'):
                        analysis['ai_category'] = fallback_result['category']
                    if fallback_result.get('title') and not analysis.get('title'):
                        analysis['title'] = fallback_result['title']
                    if fallback_result.get('summary'):
                        analysis['ai_summary'] = fallback_result['summary']
                    if fallback_result.get('tags'):
                        existing_tags = analysis.get('tags', [])
                        analysis['tags'] = list(set(existing_tags + fallback_result['tags']))
                    
                    logger.info(f"Fallback analysis applied: category={analysis.get('ai_category')}")
                    
            except Exception as e:
                logger.warning(f"Fallback analysis failed: {e}")
        
        # å¦‚æœæ˜¯æ–‡æœ¬å†…å®¹ä¸”éœ€è¦AIæ ‡é¢˜ï¼Œç”Ÿæˆæ ‡é¢˜
        if analysis.get('_needs_ai_title') and ai_available:
            if progress_callback:
                await progress_callback(lang_ctx.t('progress_ai_generating_title'), 0.62)
            try:
                content = analysis.get('content', '')
                is_forwarded = bool(message.forward_origin)
                
                # è½¬å‘æ¶ˆæ¯æ— éœ€é•¿åº¦åˆ¤æ–­ï¼Œç›´æ¥å‘é€çš„æ¶ˆæ¯éœ€è¦>=250å­—ç¬¦
                should_generate_title = is_forwarded or (content and len(content) >= 250)
                
                if should_generate_title and content:
                    # ç”Ÿæˆæ ‡é¢˜ï¼ˆæ¥æºä¿¡æ¯å·²åœ¨contentå¼€å¤´æ˜¾ç¤ºï¼Œä¸éœ€è¦åœ¨æ ‡é¢˜ä¸­é‡å¤ï¼‰
                    user_language = lang_ctx.language
                    # æ ‡é¢˜é•¿åº¦é™åˆ¶ä¸º32å­—ç¬¦
                    max_title_length = 32
                    
                    ai_title = await ai_summarizer.generate_title_from_text(
                        content, 
                        max_length=max_title_length, 
                        language=user_language
                    )
                    if ai_title:
                        analysis['title'] = ai_title
                        analysis['ai_title'] = ai_title
                        logger.info(f"AI generated title: {analysis['title']}")
                        if progress_callback:
                            await progress_callback(lang_ctx.t('progress_title_complete'), 0.65)
            except Exception as e:
                logger.warning(f"AI title generation failed: {e}")
        

        # Get storage manager
        if progress_callback:
            await progress_callback(lang_ctx.t('progress_saving_archive'), 0.7)
        
        storage_manager: StorageManager = context.bot_data.get('storage_manager')
        
        if not storage_manager:
            return False, "Storage manager not initialized"
        
        # æ·»åŠ æ¥æºä¿¡æ¯å¤´éƒ¨åˆ°content
        from ...utils.helpers import format_source_header, escape_html
        source_header = format_source_header(message, source_info)
        
        # å°†æ¥æºä¿¡æ¯æ·»åŠ åˆ°contentå¼€å¤´ï¼ˆè½¬ä¹‰ç”¨æˆ·åŸå§‹æ–‡æœ¬ï¼‰
        if analysis.get('content'):
            # source_headerå·²åŒ…å«HTMLæ ‡ç­¾ï¼Œä»…è½¬ä¹‰ç”¨æˆ·content
            user_content = escape_html(analysis['content'])
            analysis['content'] = f"{source_header}\n{user_content}"
        else:
            analysis['content'] = source_header
        
        # Archive content
        success, result_msg, archive_id = await storage_manager.archive_content(
            message, 
            analysis,
            source_info=source_info,
            is_direct_send=is_direct_send
        )
        
        if progress_callback:
            await progress_callback(lang_ctx.t('progress_complete'), 1.0)
        
        # è‡ªåŠ¨ç”Ÿæˆå…³è”ç¬”è®°ï¼ˆå¦‚æœå½’æ¡£æˆåŠŸï¼‰
        if success and archive_id:
            await _auto_generate_note(
                archive_id=archive_id,
                message=message,
                analysis=analysis,
                context=context
            )
        
        return success, result_msg, archive_id, None
        
    except Exception as e:
        if progress_callback:
            await progress_callback(lang_ctx.t('progress_failed'), 1.0)
        raise


async def _auto_generate_note(
    archive_id: int,
    message: Message,
    analysis: Dict,
    context: ContextTypes.DEFAULT_TYPE,
    user_comment: Optional[str] = None,
    original_caption: Optional[str] = None,
    source_info: Optional[Dict] = None
) -> Optional[int]:
    """
    è‡ªåŠ¨ç”Ÿæˆå…³è”ç¬”è®°
    
    æ ¹æ®å†…å®¹ç±»å‹å’Œé•¿åº¦åˆ¤æ–­æ˜¯å¦éœ€è¦ç”Ÿæˆç¬”è®°ï¼š
    1. æ–‡æœ¬å†…å®¹ >= é˜ˆå€¼ï¼šAIç”Ÿæˆç®€æ´ç¬”è®°
    2. é“¾æ¥ï¼šæ ¹æ®é“¾æ¥ä¿¡æ¯ç”Ÿæˆç¬”è®°
    3. æ–‡æ¡£ï¼ˆæœ‰AIåˆ†æï¼‰ï¼šæ•´ç†å®Œæ•´ç¬”è®°
    4. å…¶ä»–åª’ä½“ç±»å‹ï¼šæ•´åˆAIç”Ÿæˆ + ç”¨æˆ·è¯„è®º + åŸå§‹caption
    
    Args:
        archive_id: å½’æ¡£ID
        message: Telegramæ¶ˆæ¯
        analysis: å†…å®¹åˆ†æç»“æœ
        context: Bot context
        user_comment: ç”¨æˆ·é™„å¸¦çš„è¯„è®ºï¼ˆå¯é€‰ï¼‰
        original_caption: è½¬å‘æ¶ˆæ¯åŸå§‹çš„captionï¼ˆå¯é€‰ï¼‰
        source_info: æ¥æºä¿¡æ¯ï¼ŒåŒ…å«æ¥æºåç§°ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        note_id or None
    """
    try:
        note_manager = context.bot_data.get('note_manager')
        ai_summarizer = context.bot_data.get('ai_summarizer')
        
        if not note_manager:
            return None
        
        content_type = analysis.get('content_type', '')
        note_content = None
        
        # è·å–AIåˆ†æç»“æœ
        ai_summary = analysis.get('ai_summary')
        ai_category = analysis.get('ai_category')
        ai_key_points = analysis.get('ai_key_points', [])
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å¯ç”¨å†…å®¹ï¼ˆAIã€ç”¨æˆ·è¯„è®ºã€captionï¼‰
        has_ai_content = bool(ai_summary or ai_category or ai_key_points)
        has_user_comment = bool(user_comment)
        has_caption = bool(original_caption)
        
        # åªæœ‰åœ¨å®Œå…¨æ²¡æœ‰å†…å®¹æ—¶æ‰ä¸ç”Ÿæˆç¬”è®°
        if not (has_ai_content or has_user_comment or has_caption):
            logger.debug(f"No content available for note generation, skipping archive {archive_id}")
            return None
        
        # ========== ç”ŸæˆAIç¬”è®°éƒ¨åˆ† ==========
        
        # 1. æ–‡æœ¬å†…å®¹ï¼šåˆ¤æ–­é•¿åº¦ï¼Œâ‰¥é˜ˆå€¼åˆ™ç”Ÿæˆç®€æ´ç¬”è®°
        if content_type in ['text', 'article']:
            content = analysis.get('content', '')
            if content:
                from ...utils.helpers import should_create_note
                is_short, note_type = should_create_note(content)
                
                if not is_short and ai_summarizer and ai_summarizer.is_available():
                    # é•¿æ–‡æœ¬ï¼ŒAIç”Ÿæˆç®€æ´ç¬”è®°
                    from telegram import Update as TelegramUpdate
                    temp_update = TelegramUpdate(update_id=0, message=message)
                    lang_ctx = get_language_context(temp_update, context)
                    language = lang_ctx.language
                    
                    note_content = await ai_summarizer.generate_note_from_content(
                        content=content,
                        content_type='text',
                        max_length=250,
                        language=language
                    )
                    
                    if note_content:
                        note_content = f"[è‡ªåŠ¨] {note_content}"
                        logger.info(f"Auto-generated note for long text archive {archive_id}")
        
        # 2. é“¾æ¥ï¼šæ ¹æ®é“¾æ¥å…ƒæ•°æ®ç”Ÿæˆç¬”è®°
        elif content_type == 'link':
            if ai_summarizer and ai_summarizer.is_available():
                from telegram import Update as TelegramUpdate
                temp_update = TelegramUpdate(update_id=0, message=message)
                lang_ctx = get_language_context(temp_update, context)
                language = lang_ctx.language
                
                # æ„å»ºé“¾æ¥ä¿¡æ¯ç”¨äºç”Ÿæˆç¬”è®°
                link_info = f"""é“¾æ¥æ ‡é¢˜ï¼š{analysis.get('title', 'æœªçŸ¥')}
URLï¼š{analysis.get('url', '')}
"""
                # å¦‚æœæœ‰ Telegram é¢„è§ˆæ•°æ®
                telegram_preview = analysis.get('telegram_preview', {})
                if telegram_preview and telegram_preview.get('description'):
                    link_info += f"æè¿°ï¼š{telegram_preview.get('description')}\n"
                
                # ä½¿ç”¨å†…å®¹
                if analysis.get('content'):
                    link_info += f"\nå†…å®¹ï¼š\n{analysis.get('content')[:1000]}"
                
                note_content = await ai_summarizer.generate_note_from_content(
                    content=link_info,
                    content_type='link',
                    max_length=250,
                    language=language
                )
                
                if note_content:
                    note_content = f"[è‡ªåŠ¨] {note_content}"
                    logger.info(f"Auto-generated note for link archive {archive_id}")

        
        # 3. æ–‡æ¡£ï¼šå¦‚æœæœ‰AIåˆ†æç»“æœï¼Œæ•´ç†å®Œæ•´ç¬”è®°
        elif content_type in ['document', 'ebook']:
            if has_ai_content and ai_summarizer and ai_summarizer.is_available():
                from telegram import Update as TelegramUpdate
                temp_update = TelegramUpdate(update_id=0, message=message)
                lang_ctx = get_language_context(temp_update, context)
                language = lang_ctx.language
                
                title = analysis.get('title') or analysis.get('file_name', 'æœªçŸ¥æ–‡æ¡£')
                
                note_content = await ai_summarizer.generate_note_from_ai_analysis(
                    ai_summary=ai_summary,
                    ai_key_points=ai_key_points,
                    ai_category=ai_category,
                    title=title,
                    language=language
                )
                
                if note_content:
                    note_content = f"[è‡ªåŠ¨] {note_content}"
                    logger.info(f"Auto-generated note for document archive {archive_id}")
        
        # 4. å…¶ä»–ç±»å‹ï¼ˆå›¾ç‰‡ã€è§†é¢‘ã€éŸ³é¢‘ç­‰ï¼‰ï¼šå¦‚æœæœ‰AIåˆ†æï¼Œç”Ÿæˆç¬”è®°
        else:
            if has_ai_content and ai_summarizer and ai_summarizer.is_available():
                from telegram import Update as TelegramUpdate
                temp_update = TelegramUpdate(update_id=0, message=message)
                lang_ctx = get_language_context(temp_update, context)
                language = lang_ctx.language
                
                # ä½¿ç”¨AIæ‘˜è¦ç”Ÿæˆç¬”è®°
                note_content = await ai_summarizer.generate_note_from_content(
                    content=ai_summary or ai_category or '',
                    content_type=content_type,
                    max_length=250,
                    language=language
                )
                
                if note_content:
                    note_content = f"[è‡ªåŠ¨] {note_content}"
                    logger.info(f"Auto-generated note for {content_type} archive {archive_id}")
        
        # æ„å»ºå®Œæ•´çš„ç¬”è®°å†…å®¹ï¼šAIç”Ÿæˆ + ç”¨æˆ·è¯„è®º + åŸå§‹caption
        if note_content or user_comment or original_caption:
            final_note_parts = []
            
            # ç¬¬ä¸€éƒ¨åˆ†ï¼šAIç”Ÿæˆçš„ç¬”è®°
            if note_content:
                final_note_parts.append(note_content)
            
            # ç¬¬äºŒéƒ¨åˆ†ï¼šç”¨æˆ·è¯„è®º
            if user_comment:
                # åªæœ‰å‰é¢æœ‰å†…å®¹æ—¶æ‰æ·»åŠ åˆ†éš”çº¿
                if final_note_parts:
                    final_note_parts.append("----------------------------------")
                final_note_parts.append(f"[ç”¨æˆ·]: {user_comment}")
            
            # ç¬¬ä¸‰éƒ¨åˆ†ï¼šåŸå§‹captionï¼ˆè½¬å‘æ¶ˆæ¯çš„åŸæ–‡ï¼‰
            if original_caption:
                # åªæœ‰å‰é¢æœ‰å†…å®¹æ—¶æ‰æ·»åŠ åˆ†éš”çº¿
                if final_note_parts:
                    final_note_parts.append("----------------------------------")
                final_note_parts.append(f"[åŸæ–‡]: {original_caption}")
            
            # åˆå¹¶æ‰€æœ‰éƒ¨åˆ†
            final_note_content = "\n".join(final_note_parts)
            
            # æå–ç¬”è®°æ ‡é¢˜ï¼šä¼˜å…ˆä½¿ç”¨AIåˆ†æçš„æ ‡é¢˜æˆ–æ–‡ä»¶å
            note_title = None
            
            # ç»Ÿä¸€çš„æ ‡é¢˜æå–é€»è¾‘ï¼šä¼˜å…ˆä½¿ç”¨ analysis.titleï¼Œå…¶æ¬¡ file_nameï¼Œæœ€ååŸå§‹caption
            if analysis.get('title'):
                note_title = analysis.get('title')
            elif analysis.get('file_name'):
                note_title = analysis.get('file_name')
            elif original_caption:
                # æ–‡æœ¬ç±»å‹æˆ–å…¶ä»–æ²¡æœ‰æ ‡é¢˜çš„å†…å®¹ï¼Œå°è¯•ä»captionè·å–
                note_title = original_caption[:50]
            
            # ä¿å­˜ç¬”è®°
            note_id = note_manager.add_note(archive_id, final_note_content)
            if note_id:
                logger.info(f"Auto-generated note {note_id} for archive {archive_id} (with_user_comment={bool(user_comment)}, with_caption={bool(original_caption)})")
                
                # è½¬å‘ç¬”è®°åˆ°Telegramé¢‘é“ï¼ˆä¸æ‰‹åŠ¨ç¬”è®°æ¨¡å¼ä¿æŒä¸€è‡´ï¼‰
                from ...utils.note_storage_helper import forward_note_to_channel, update_archive_message_buttons
                storage_path = await forward_note_to_channel(
                    context=context,
                    note_id=note_id,
                    note_content=final_note_content,
                    note_title=note_title,
                    note_manager=note_manager
                )
                
                if storage_path:
                    logger.info(f"Auto-generated note {note_id} forwarded to channel: {storage_path}")
                
                # æ›´æ–°åŸå§‹å­˜æ¡£æ¶ˆæ¯çš„æŒ‰é’®ï¼ˆå°†"æ·»åŠ ç¬”è®°"æ”¹ä¸º"æŸ¥çœ‹ç¬”è®°"ï¼‰
                await update_archive_message_buttons(context, archive_id)
                
                return note_id
        
        return None
        
    except Exception as e:
        logger.error(f"Error auto-generating note: {e}", exc_info=True)
        return None
