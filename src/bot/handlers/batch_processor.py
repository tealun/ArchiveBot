"""
Batch message processor
"""

import logging
import time
from typing import List, Optional, Dict
from telegram import Update, Message
from telegram.ext import ContextTypes
from telegram.constants import ParseMode

from ...utils.language_context import get_language_context
from ...utils.helpers import format_file_size, truncate_text, extract_hashtags
from .message_processor import _process_single_message, _auto_generate_note

logger = logging.getLogger(__name__)

from ...core.analyzer import ContentAnalyzer
from ...core.storage_manager import StorageManager


def _should_silent_archive(source_info: Optional[Dict], config) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦åº”è¯¥é™é»˜å½’æ¡£ï¼ˆä¸å›å¤æ¶ˆæ¯ä¸”åˆ é™¤è½¬å‘æ¶ˆæ¯ï¼‰
    
    Args:
        source_info: æ¥æºä¿¡æ¯ {'name': str, 'id': int, 'type': str}
        config: é…ç½®å¯¹è±¡
        
    Returns:
        bool: Trueè¡¨ç¤ºåº”è¯¥é™é»˜å¤„ç†
    """
    if not source_info:
        return False
    
    silent_sources = config.get('bot.silent_sources', [])
    if not silent_sources:
        return False
    
    source_id = source_info.get('id')
    source_name = source_info.get('name', '')
    
    for silent_source in silent_sources:
        # æ”¯æŒIDåŒ¹é…ï¼ˆæ•´æ•°æˆ–å­—ç¬¦ä¸²å½¢å¼ï¼‰
        if isinstance(silent_source, (int, str)):
            try:
                # å°è¯•è½¬æ¢ä¸ºæ•´æ•°è¿›è¡ŒIDåŒ¹é…
                silent_id = int(str(silent_source).replace('-100', '').replace('-', ''))
                check_id = int(str(source_id).replace('-100', '').replace('-', ''))
                if silent_id == check_id:
                    return True
            except (ValueError, TypeError):
                # å¦‚æœä¸æ˜¯æ•°å­—ï¼Œè¿›è¡Œç”¨æˆ·ååŒ¹é…
                silent_username = str(silent_source).strip().lstrip('@')
                source_username = source_name.strip().lstrip('@')
                if silent_username.lower() == source_username.lower():
                    return True
    
    return False



async def _process_batch_messages(messages: List[Message], context: ContextTypes.DEFAULT_TYPE, merged_caption: Optional[str] = None, source_info: Optional[Dict] = None, is_forwarded: bool = False, progress_callback=None) -> List[tuple]:
    """
    æ‰¹é‡å¤„ç†æ¶ˆæ¯ï¼ˆä¼˜åŒ–ï¼šå…±äº«æ‰‹åŠ¨æ ‡ç­¾ + ç‹¬ç«‹AIæ ‡ç­¾ï¼‰
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        context: Bot context
        merged_caption: åˆå¹¶çš„captionæ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
        source_info: æ¥æºä¿¡æ¯ï¼ˆä»batchä¸­æå–ï¼‰
        is_forwarded: æ˜¯å¦ä¸ºè½¬å‘æ¶ˆæ¯
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, stage)
        
    Returns:
        [(success, result_msg), ...]
    """
    # ä»ç¬¬ä¸€æ¡ message åˆ›å»ºä¸´æ—¶ update å¯¹è±¡ä»¥è·å–è¯­è¨€ä¸Šä¸‹æ–‡
    from telegram import Update as TelegramUpdate
    temp_update = TelegramUpdate(update_id=0, message=messages[0])
    lang_ctx = get_language_context(temp_update, context)
    results = []
    total = len(messages)
    
    # é˜¶æ®µ1: åˆ†æå†…å®¹ (0-20%)
    if progress_callback:
        await progress_callback(0, total, lang_ctx.t('batch_progress_analyzing'))
    
    analyses = []
    for i, message in enumerate(messages):
        # åŸºç¡€åˆ†æ
        analysis = ContentAnalyzer.analyze(message)
        
        # linkç±»å‹ä½¿ç”¨å¼‚æ­¥æ·±åº¦åˆ†æï¼ˆWebArchiverï¼‰
        if analysis.get('content_type') == 'link':
            try:
                logger.debug(f"Link detected in batch message {i}, attempting WebArchiver analysis...")
                analysis = await ContentAnalyzer.analyze_async(message)
            except Exception as e:
                logger.warning(f"Async analyze failed for batch message {i}: {e}")
                # å·²ç»æœ‰åŸºç¡€analysisï¼Œç»§ç»­ä½¿ç”¨
        
        analyses.append(analysis)
        if progress_callback and (i + 1) % max(1, total // 10) == 0:
            await progress_callback(i + 1, total, lang_ctx.t('batch_progress_analyzing'))
    
    # é˜¶æ®µ2: æå–å…±äº«æ ‡ç­¾ (20%)
    if progress_callback:
        await progress_callback(total, total, lang_ctx.t('batch_progress_extracting_tags'))
    
    # æå–å…±äº«çš„hashtagsï¼ˆä»merged_captionï¼‰- è¿™æ˜¯ç”¨æˆ·ä¸»åŠ¨è¾“å…¥çš„æ ‡ç­¾
    shared_hashtags = []
    if merged_caption:
        shared_hashtags = extract_hashtags(merged_caption)
        logger.info(f"Extracted shared hashtags from caption: {shared_hashtags}")
    
    # é˜¶æ®µ3: AIå¤„ç† (20-50%) - æ‰¹é‡æ¶ˆæ¯åªåˆ†æä¸€æ¬¡åˆå¹¶çš„caption
    if progress_callback:
        await progress_callback(0, total, lang_ctx.t('batch_progress_ai_generating_tags'))
    
    # æ‰¹é‡AIå¤„ç† - åªå¯¹åˆå¹¶çš„caption+ç”¨æˆ·è¯„è®ºè°ƒç”¨ä¸€æ¬¡AI
    shared_ai_result = {'tags': [], 'title': None, 'summary': None}
    ai_summarizer = context.bot_data.get('ai_summarizer')
    if ai_summarizer and ai_summarizer.is_available():
        from ...utils.config import get_config
        config = get_config()
        
        # åªåˆ†æä¸€æ¬¡ï¼šä½¿ç”¨merged_captionï¼ˆåŒ…å«ç”¨æˆ·è¯„è®ºï¼‰
        if config.ai.get('auto_generate_tags', False) and merged_caption:
            try:
                start = time.time()
                max_tags = config.ai.get('max_generated_tags', 8)
                max_tags = max(3, min(max_tags, 5))  # æ‰¹é‡æ—¶é™åˆ¶åœ¨3-5ä¹‹é—´
                
                # ç”Ÿæˆæ ‡ç­¾
                ai_tags = await ai_summarizer.generate_tags(merged_caption, max_tags, language=lang_ctx.language)
                duration = time.time() - start
                provider = getattr(ai_summarizer, '_last_call_info', {}).get('provider', 'single')
                logger.info(f"Batch AI single analysis: provider={provider}, duration={duration:.2f}s, tags={ai_tags}")
                
                if ai_tags:
                    shared_ai_result['tags'] = ai_tags
                
                # ç”Ÿæˆæ ‡é¢˜ï¼ˆé™åˆ¶32å­—ç¬¦ï¼‰
                if config.ai.get('auto_generate_title', False):
                    ai_title = await ai_summarizer.generate_title(merged_caption, language=lang_ctx.language)
                    if ai_title:
                        shared_ai_result['title'] = ai_title[:32]
                        logger.info(f"Batch AI generated title: {shared_ai_result['title']}")
                
                # ç”Ÿæˆæ‘˜è¦ï¼ˆæ£€æŸ¥å†…å®¹é•¿åº¦æ˜¯å¦è¾¾åˆ°é˜ˆå€¼ï¼‰
                min_length = config.ai.get('min_content_length_for_summary', 150)
                if config.ai.get('auto_summarize', False) and len(merged_caption) >= min_length:
                    ai_summary_result = await ai_summarizer.summarize_content(merged_caption, language=lang_ctx.language)
                    if ai_summary_result and ai_summary_result.get('success'):
                        summary_text = ai_summary_result.get('summary', '')
                        if summary_text:
                            shared_ai_result['summary'] = summary_text
                            logger.info(f"Batch AI generated summary: {len(summary_text)} chars")
                
                logger.info(f"Batch AI shared analysis completed")
            except Exception as e:
                logger.warning(f"Batch AI analysis failed: {e}")
    
    if progress_callback:
        await progress_callback(total, total, lang_ctx.t('batch_progress_ai_generating_tags'))
    
    # é˜¶æ®µ4: åº”ç”¨æ ‡ç­¾å’ŒAIç»“æœ (50-60%)
    if progress_callback:
        await progress_callback(0, total, lang_ctx.t('batch_progress_applying_tags'))
    
    # åº”ç”¨å…±äº«çš„AIåˆ†æç»“æœå’Œhashtagsåˆ°æ‰€æœ‰åˆ†æç»“æœ
    for i, analysis in enumerate(analyses):
        # æ·»åŠ æ¥æºä¿¡æ¯å¤´éƒ¨åˆ°æ¯æ¡æ¶ˆæ¯
        from ...utils.helpers import format_source_header
        source_header = format_source_header(messages[i], source_info)
        
        # å°†æ¥æºä¿¡æ¯æ·»åŠ åˆ°contentå¼€å¤´
        if analysis.get('content'):
            analysis['content'] = f"{source_header}\n{analysis['content']}"
        else:
            analysis['content'] = source_header
        
        # æ·»åŠ å…±äº«çš„hashtagsï¼ˆç”¨æˆ·æ‰‹åŠ¨æ ‡ç­¾ï¼‰
        if shared_hashtags:
            existing_hashtags = analysis.get('hashtags', [])
            analysis['hashtags'] = list(set(existing_hashtags + shared_hashtags))
        
        # æ·»åŠ å…±äº«çš„AIæ ‡ç­¾åˆ°æ‰€æœ‰item
        if shared_ai_result['tags']:
            existing_tags = analysis.get('tags', [])
            analysis['tags'] = list(set(existing_tags + shared_ai_result['tags']))
        
        # ä½¿ç”¨å…±äº«çš„AIæ ‡é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦åˆ™æˆªå–captionå‰32å­—ç¬¦
        if shared_ai_result['title']:
            analysis['title'] = shared_ai_result['title']
        elif merged_caption and not analysis.get('title'):
            # æˆªå–captionå‰32å­—ç¬¦ä½œä¸ºæ ‡é¢˜
            analysis['title'] = merged_caption[:32] + ('...' if len(merged_caption) > 32 else '')
        
        # ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼šå¦‚æœæœ‰merged_captionä¸”ä¸åŸå§‹contentä¸åŒï¼Œæ·»åŠ æ‰¹æ³¨æ ‡è®°
        # æ³¨æ„ï¼šmerged_captioné€šå¸¸å°±æ˜¯captionæœ¬èº«ï¼Œåªåœ¨æœ‰ç”¨æˆ·é¢å¤–è¯„è®ºæ—¶æ‰ä¸åŒ
        if i == 0 and merged_caption:
            existing_content = analysis.get('content', '')
            # å¦‚æœcontentä¸­è¿˜æ²¡æœ‰åŒ…å«merged_captionï¼Œæ‰æ·»åŠ æ‰¹æ³¨
            # æˆ–è€…ï¼Œå¦‚æœmerged_captionæ˜¯ç”¨æˆ·è¯„è®ºï¼ˆä¸åŒäºåŸå§‹captionï¼‰ï¼Œä¹Ÿæ·»åŠ 
            if merged_caption not in existing_content:
                analysis['content'] = f"{existing_content}\nğŸ“ æ‰¹æ³¨: {merged_caption}"
    
    if progress_callback:
        await progress_callback(total, total, lang_ctx.t('batch_progress_applying_tags'))
    
    logger.info(f"Batch processing: shared hashtags={shared_hashtags}, each item has independent AI tags")
    
    # é˜¶æ®µ5: æ‰¹é‡å­˜å‚¨ (60-100%)
    if progress_callback:
        await progress_callback(0, total, lang_ctx.t('batch_progress_storing'))
    
    # æ‰¹é‡å­˜å‚¨ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨storage_managerçš„æ‰¹é‡æ–¹æ³•ï¼‰
    storage_manager: StorageManager = context.bot_data.get('storage_manager')
    if not storage_manager:
        return [(False, "Storage manager not initialized", None) for _ in messages]
    
    # è°ƒç”¨æ‰¹é‡å½’æ¡£ï¼ˆå¸¦è¿›åº¦å›è°ƒå’Œæ¥æºä¿¡æ¯ï¼‰
    results = await storage_manager.batch_archive_content(
        messages, 
        analyses, 
        source_info=source_info,
        is_batch_forwarded=is_forwarded,
        progress_callback=progress_callback
    )
    
    if progress_callback:
        await progress_callback(total, total, lang_ctx.t('batch_progress_complete'))
    
    # ä¸ºæ¯ä¸ªæˆåŠŸå½’æ¡£çš„æ¶ˆæ¯ç”Ÿæˆç¬”è®°
    # æå–æ‰¹é‡ç”¨æˆ·è¯„è®ºå’ŒåŸå§‹captionï¼ˆæ¥è‡ªbatch aggregatorï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä»context.user_dataæˆ–é€šè¿‡å‚æ•°ä¼ é€’æ‰¹æ¬¡ä¿¡æ¯
    # ç”±äºæ‰¹é‡å¤„ç†çš„ç‰¹æ€§ï¼Œæˆ‘ä»¬éœ€è¦åœ¨è°ƒç”¨è¿™ä¸ªå‡½æ•°æ—¶ä¼ é€’è¿™äº›ä¿¡æ¯
    
    return results


async def _batch_callback(messages: List[Message], merged_caption: Optional[str], source_info: Optional[Dict], is_forwarded: bool, update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    æ‰¹æ¬¡å¤„ç†å›è°ƒ
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        merged_caption: åˆå¹¶çš„caption
        source_info: æ¥æºä¿¡æ¯
        is_forwarded: æ˜¯å¦ä¸ºè½¬å‘æ¶ˆæ¯
        update: Telegram update
        context: Bot context
    """
    lang_ctx = get_language_context(update, context)
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥é™é»˜å½’æ¡£
    from ...utils.config import get_config
    config = get_config()
    should_silent = _should_silent_archive(source_info, config)
    
    try:
        if len(messages) == 1:
            # å•æ¡æ¶ˆæ¯å¤„ç†
            message = messages[0]
            
            # é™é»˜æ¨¡å¼ï¼šä¸å‘é€è¿›åº¦æ¶ˆæ¯
            processing_msg = None if should_silent else await message.reply_text(lang_ctx.t('archive_processing'))
            
            try:
                # å®šä¹‰è¿›åº¦æ›´æ–°å›è°ƒ
                async def update_progress(stage: str, progress: float):
                    if should_silent or not processing_msg:
                        return  # é™é»˜æ¨¡å¼ä¸æ›´æ–°è¿›åº¦
                    try:
                        percentage = int(progress * 100)
                        progress_bar = 'â–ˆ' * (percentage // 5) + 'â–‘' * (20 - percentage // 5)
                        await processing_msg.edit_text(
                            f"â³ {stage}\n"
                            f"{lang_ctx.t('archive_progress', percentage=percentage)}\n"
                            f"{progress_bar}"
                        )
                    except Exception as e:
                        logger.debug(f"Progress update failed: {e}")
                
                success, result_msg, archive_id, duplicate_info = await _process_single_message(message, context, merged_caption, update_progress)
                
                # é™é»˜æ¨¡å¼ï¼šå¤„ç†å®Œæˆååˆ é™¤è½¬å‘æ¶ˆæ¯å¹¶è¿”å›
                if should_silent:
                    try:
                        await message.delete()
                        logger.info(f"Silent archive: deleted forwarded message from {source_info.get('name')}")
                    except Exception as e:
                        logger.warning(f"Failed to delete forwarded message: {e}")
                    return
                
                # å¦‚æœæ£€æµ‹åˆ°é‡å¤æ–‡ä»¶ï¼Œæ„å»ºå¹¶å‘é€é‡å¤æç¤ºæ¶ˆæ¯
                if duplicate_info and processing_msg:
                    # æ„å»ºé‡å¤æ–‡ä»¶æç¤ºæ¶ˆæ¯ï¼ˆä½¿ç”¨HTMLæ ¼å¼ï¼‰
                    dup_msg = f"{lang_ctx.t('archive_duplicate_file')}\n\n"
                    
                    # ä¼˜åŒ–æ ‡é¢˜æ˜¾ç¤ºï¼šä¼˜å…ˆä½¿ç”¨titleï¼Œå¦åˆ™æˆªå–content/captionï¼ˆæœ€å¤š50å­—ç¬¦ï¼‰
                    raw_title = duplicate_info.get('title', '')
                    raw_content = duplicate_info.get('content', '')
                    
                    # ç¡®å®šæ˜¾ç¤ºçš„æ ‡é¢˜
                    if raw_title and len(raw_title) <= 50:
                        file_title = raw_title
                    elif raw_title:
                        file_title = raw_title[:50] + '...'
                    elif raw_content:
                        # æ²¡æœ‰titleï¼Œä½¿ç”¨contentå‰50å­—ç¬¦
                        file_title = raw_content[:50] + ('...' if len(raw_content) > 50 else '')
                    else:
                        file_title = lang_ctx.t('archive_duplicate_unknown_title')
                    
                    storage_path = duplicate_info.get('storage_path')
                    storage_type = duplicate_info.get('storage_type')
                    
                    # æ„å»ºå¸¦é“¾æ¥çš„æ–‡ä»¶å
                    if storage_path and storage_type == 'telegram':
                        from ...utils.config import get_config
                        config = get_config()
                        channel_id = config.telegram_channel_id
                        if channel_id:
                            # è§£æ storage_path: å¯èƒ½æ˜¯ "message_id" æˆ– "channel_id:message_id" æˆ– "channel_id:message_id:file_id"
                            parts = storage_path.split(':')
                            if len(parts) >= 2:
                                # æ ¼å¼: channel_id:message_id[:file_id]
                                channel_id_str = parts[0].replace('-100', '')
                                message_id = parts[1]
                            else:
                                # æ ¼å¼: message_idï¼ˆä½¿ç”¨é…ç½®çš„channel_idï¼‰
                                channel_id_str = str(channel_id).replace('-100', '')
                                message_id = storage_path
                            
                            file_link = f"https://t.me/c/{channel_id_str}/{message_id}"
                            dup_msg += lang_ctx.t('archive_duplicate_file_name', title=f"<a href='{file_link}'>{file_title}</a>") + "\n"
                        else:
                            dup_msg += lang_ctx.t('archive_duplicate_file_name', title=file_title) + "\n"
                    else:
                        dup_msg += lang_ctx.t('archive_duplicate_file_name', title=file_title) + "\n"
                    
                    dup_msg += lang_ctx.t('archive_duplicate_file_size', size=format_file_size(duplicate_info.get('file_size', 0))) + "\n"
                    dup_msg += lang_ctx.t('archive_duplicate_file_archived_at', time=duplicate_info.get('archived_at', lang_ctx.t('archive_duplicate_unknown_time'))) + "\n"
                    
                    # è·å–æ ‡ç­¾
                    tag_manager = context.bot_data.get('tag_manager')
                    if tag_manager:
                        tags = tag_manager.get_archive_tags(duplicate_info['id'])
                        if tags:
                            tag_str = ' '.join([f"#{tag}" for tag in tags])
                            dup_msg += lang_ctx.t('archive_duplicate_file_tags', tags=tag_str)
                    
                    await processing_msg.edit_text(dup_msg, parse_mode='HTML')
                    return
                
                # å¦‚æœå½’æ¡£æˆåŠŸä¸”æœ‰archive_idï¼Œæ·»åŠ æ“ä½œæŒ‰é’®ï¼ˆåŒ…å«ç²¾ç‚¼ç¬”è®°ï¼‰
                if success and archive_id and processing_msg:
                    from telegram import InlineKeyboardButton, InlineKeyboardMarkup
                    
                    # è·å–æ•°æ®åº“æ£€æŸ¥çŠ¶æ€
                    db_storage = context.bot_data.get('db_storage')
                    db = db_storage.db if db_storage else None
                    
                    has_notes = db.has_notes(archive_id) if db else False
                    is_favorite = db.is_favorite(archive_id) if db else False
                    
                    note_icon = "ğŸ“âœ“" if has_notes else "ğŸ“"
                    fav_icon = "â¤ï¸" if is_favorite else "ğŸ¤"
                    
                    keyboard = [[
                        InlineKeyboardButton(note_icon, callback_data=f"note:{archive_id}"),
                        InlineKeyboardButton(fav_icon, callback_data=f"fav:{archive_id}"),
                        InlineKeyboardButton("â†—ï¸", callback_data=f"forward:{archive_id}")
                    ]]
                    
                    # å¦‚æœæœ‰ç¬”è®°ï¼Œæ·»åŠ "ç²¾ç‚¼ç¬”è®°"æŒ‰é’®
                    if has_notes:
                        keyboard.append([
                            InlineKeyboardButton("âœ¨ ç²¾ç‚¼ç¬”è®°", callback_data=f"refine_note:{archive_id}")
                        ])
                    
                    reply_markup = InlineKeyboardMarkup(keyboard)
                    await processing_msg.edit_text(result_msg, parse_mode='HTML', reply_markup=reply_markup)
                elif processing_msg:
                    # ä½¿ç”¨HTMLè§£ææ¨¡å¼ï¼ˆå› ä¸ºresult_msgå¯èƒ½åŒ…å«HTMLé“¾æ¥ï¼‰
                    await processing_msg.edit_text(result_msg, parse_mode='HTML')
                
                if success:
                    logger.info(f"Message archived: type={ContentAnalyzer.analyze(message).get('content_type')}")
            
            finally:
                # ç¡®ä¿è¿›åº¦æ¶ˆæ¯è¢«åˆ é™¤ï¼ˆå¦‚æœè¿˜å­˜åœ¨ä¸”æœªè¢«ä¿®æ”¹ä¸ºæœ€ç»ˆæ¶ˆæ¯ï¼‰
                # è¿™é‡Œåªæ˜¯å…œåº•ä¿æŠ¤ï¼Œæ­£å¸¸æƒ…å†µä¸‹æ¶ˆæ¯å·²åœ¨ä¸Šé¢è¢«editä¸ºæœ€ç»ˆçŠ¶æ€
                pass
        else:
            # æ‰¹é‡æ¶ˆæ¯å¤„ç†
            first_message = messages[0]
            
            # é™é»˜æ¨¡å¼ï¼šä¸å‘é€è¿›åº¦æ¶ˆæ¯
            processing_msg = None if should_silent else await first_message.reply_text(
                lang_ctx.t('batch_processing_start', total=len(messages))
            )
            
            # å®šä¹‰è¿›åº¦æ›´æ–°å›è°ƒ
            last_update_time = [0]  # ä½¿ç”¨åˆ—è¡¨å­˜å‚¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹
            
            async def update_progress(current, total, stage):
                """æ›´æ–°è¿›åº¦æ¶ˆæ¯"""
                if should_silent or not processing_msg:
                    return  # é™é»˜æ¨¡å¼ä¸æ›´æ–°è¿›åº¦
                    
                nonlocal last_update_time
                current_time = time.time()
                
                # é™åˆ¶æ›´æ–°é¢‘ç‡ï¼ˆæ¯0.5ç§’æœ€å¤šæ›´æ–°ä¸€æ¬¡ï¼‰
                if current_time - last_update_time[0] < 0.5 and current < total:
                    return
                
                last_update_time[0] = current_time
                percentage = int((current / total) * 100) if total > 0 else 0
                
                try:
                    await processing_msg.edit_text(
                        f"ğŸ“¦ æ‰¹é‡å¤„ç†ä¸­\n"
                        f"é˜¶æ®µ: {stage}\n"
                        f"è¿›åº¦: {current}/{total} ({percentage}%)"
                    )
                except Exception as e:
                    logger.debug(f"Progress update failed: {e}")
            
            # è°ƒç”¨æ‰¹é‡å¤„ç†ï¼ˆä¼ é€’source_infoå’Œis_forwardedï¼‰
            results = await _process_batch_messages(
                messages, 
                context, 
                merged_caption,
                source_info=source_info,
                is_forwarded=is_forwarded,
                progress_callback=update_progress
            )
            
            # ä¸ºæ‰¹é‡å½’æ¡£ç”Ÿæˆå…±äº«çš„ç¬”è®°
            # æ‰¹é‡æ¶ˆæ¯åº”è¯¥å…±äº«ä¸€ä¸ªç¬”è®°ï¼Œå…³è”åˆ°ç¬¬ä¸€ä¸ªæˆåŠŸçš„å½’æ¡£
            if results:
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæˆåŠŸçš„å½’æ¡£
                first_success_archive_id = None
                for success, msg, archive_id in results:
                    if success and archive_id:
                        first_success_archive_id = archive_id
                        break
                
                if first_success_archive_id:
                    # ä»batchä¸­æå–ç”¨æˆ·è¯„è®ºå’ŒåŸå§‹caption
                    # éœ€è¦åŒºåˆ†ç”¨æˆ·çš„è¯„è®ºæ–‡æœ¬å’Œåª’ä½“æ¶ˆæ¯è‡ªå¸¦çš„caption
                    user_comment = None
                    original_caption = None
                    
                    # æŸ¥æ‰¾ç”¨æˆ·è‡ªå·±å‘é€çš„æ–‡æœ¬æ¶ˆæ¯ï¼ˆéè½¬å‘ï¼Œéåª’ä½“ï¼‰
                    for msg in messages:
                        if msg.text and not msg.forward_origin and not any([
                            msg.photo, msg.video, msg.document,
                            msg.audio, msg.voice, msg.animation
                        ]):
                            user_comment = msg.text
                            break
                    
                    # æŸ¥æ‰¾åª’ä½“æ¶ˆæ¯è‡ªå¸¦çš„caption
                    for msg in messages:
                        if msg.caption:
                            original_caption = msg.caption
                            break
                    
                    # å¦‚æœç”¨æˆ·è¯„è®ºå°±æ˜¯merged_captionä¸”æ²¡æœ‰å…¶ä»–captionï¼Œåˆ™åªä¿ç•™ç”¨æˆ·è¯„è®º
                    if user_comment == merged_caption and not original_caption:
                        pass  # ä¿æŒuser_commentï¼Œä¸éœ€è¦é¢å¤–å¤„ç†
                    elif not user_comment and merged_caption:
                        # merged_captionå¯èƒ½æ˜¯ç”¨æˆ·è¯„è®º
                        user_comment = merged_caption
                    
                    # ä¸ºç¬¬ä¸€ä¸ªå½’æ¡£ç”Ÿæˆå…±äº«ç¬”è®°ï¼ˆåŒ…å«AIç”Ÿæˆ+ç”¨æˆ·è¯„è®º+åŸå§‹captionï¼‰
                    await _auto_generate_note(
                        archive_id=first_success_archive_id,
                        message=messages[0],
                        analysis=ContentAnalyzer.analyze(messages[0]),
                        context=context,
                        user_comment=user_comment,
                        original_caption=original_caption,
                        source_info=source_info
                    )
                    logger.info(f"Generated shared note for batch, linked to archive {first_success_archive_id}")
            
            # é™é»˜æ¨¡å¼ï¼šå¤„ç†å®Œæˆååˆ é™¤æ‰€æœ‰è½¬å‘æ¶ˆæ¯å¹¶è¿”å›
            if should_silent:
                for msg in messages:
                    try:
                        await msg.delete()
                    except Exception as e:
                        logger.warning(f"Failed to delete forwarded message: {e}")
                logger.info(f"Silent archive: deleted {len(messages)} forwarded messages from {source_info.get('name')}")
                return
            
            # ç»Ÿè®¡ç»“æœ
            success_count = sum(1 for success, _, _ in results if success)
            fail_count = len(results) - success_count
            
            # æ”¶é›†å½’æ¡£IDå’Œè¯¦ç»†ä¿¡æ¯
            archive_ids = [archive_id for success, _, archive_id in results if success and archive_id]
            first_id = min(archive_ids) if archive_ids else 0
            last_id = max(archive_ids) if archive_ids else 0
            
            # ç»Ÿè®¡å†…å®¹ç±»å‹
            type_counts = {}
            for i, (success, _, _) in enumerate(results):
                if success and i < len(messages):
                    analysis = ContentAnalyzer.analyze(messages[i])
                    content_type = analysis.get('content_type', 'unknown')
                    type_name = lang_ctx.t(f'content_type_{content_type}', default=content_type)
                    type_counts[type_name] = type_counts.get(type_name, 0) + 1
            
            # æ ¼å¼åŒ–å†…å®¹ç±»å‹ç»Ÿè®¡
            types_str = '\n'.join([f"  â€¢ {t}: {c} æ¡" for t, c in type_counts.items()])
            
            # è·å–æ ‡ç­¾ï¼ˆä»ç¬¬ä¸€ä¸ªæˆåŠŸçš„å½’æ¡£ï¼‰
            tags_str = "æ— "
            tag_manager = context.bot_data.get('tag_manager')
            if tag_manager and archive_ids:
                tags = tag_manager.get_archive_tags(archive_ids[0])
                if tags:
                    # é™åˆ¶æ˜¾ç¤ºå‰5ä¸ªæ ‡ç­¾
                    display_tags = tags[:5]
                    tags_str = ' '.join([f"#{tag}" for tag in display_tags])
                    if len(tags) > 5:
                        tags_str += f" (+{len(tags) - 5})"
            
            # æ¥æºä¿¡æ¯
            source_str = ""
            if source_info:
                source_str = lang_ctx.t('batch_source_from', source=source_info.get('name', 'æœªçŸ¥'))
            else:
                source_str = lang_ctx.t('batch_source_direct')
            
            # AIåˆ†æç»“æœï¼ˆä»ç¬¬ä¸€ä¸ªæˆåŠŸçš„å½’æ¡£è·å–ï¼‰
            ai_summary_str = ""
            if archive_ids:
                db_storage = context.bot_data.get('db_storage')
                if db_storage:
                    # ç›´æ¥ä»æ•°æ®åº“æŸ¥è¯¢AIæ‘˜è¦
                    try:
                        result = db_storage.db.execute(
                            "SELECT ai_summary FROM archives WHERE id = ? AND deleted = 0",
                            (archive_ids[0],)
                        ).fetchone()
                        if result and result[0]:
                            summary = result[0]
                            # é™åˆ¶æ‘˜è¦é•¿åº¦ï¼Œé¿å…æ¶ˆæ¯è¿‡é•¿
                            max_len = 150
                            if len(summary) > max_len:
                                summary = summary[:max_len] + '...'
                            ai_summary_str = f"\n\nğŸ¤– AIæ‘˜è¦:\n{summary}"
                    except Exception as e:
                        logger.debug(f"Failed to fetch AI summary: {e}")
            
            if fail_count > 0:
                summary_msg = lang_ctx.t('batch_processing_complete', 
                                        success=success_count, 
                                        fail=fail_count,
                                        first_id=first_id,
                                        last_id=last_id,
                                        types=types_str,
                                        tags=tags_str,
                                        source=source_str) + ai_summary_str
            else:
                summary_msg = lang_ctx.t('batch_processing_complete_no_fail', 
                                        success=success_count,
                                        first_id=first_id,
                                        last_id=last_id,
                                        types=types_str,
                                        tags=tags_str,
                                        source=source_str) + ai_summary_str
            
            if processing_msg:
                await processing_msg.edit_text(summary_msg)
            logger.info(f"Batch archived: {success_count}/{len(messages)} messages")
            
    except Exception as e:
        logger.error(f"Error in batch callback: {e}", exc_info=True)
