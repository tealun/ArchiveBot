"""
AI Data Cache Manager

è½»é‡çº§ç¼“å­˜ç®¡ç†ï¼Œä¼˜åŒ–å†…å­˜æ¶ˆè€—
é‡‡ç”¨ï¼š
1. çŸ­TTLï¼ˆå‡å°‘è¿‡æœŸæ•°æ®é©»ç•™ï¼‰
2. LRUç­–ç•¥ï¼ˆé™åˆ¶ç¼“å­˜å®¹é‡ï¼‰
3. ä»…ç¼“å­˜ç»“æœè€Œéå®Œæ•´æ•°æ®
"""
import logging
import time
from typing import Dict, Any, Optional, List
from threading import Lock
from collections import OrderedDict

logger = logging.getLogger(__name__)


class LRUCache:
    """ç®€å•çš„LRUç¼“å­˜å®ç°"""
    
    def __init__(self, capacity: int = 10):
        self.cache = OrderedDict()
        self.capacity = capacity
        self.lock = Lock()
    
    def get(self, key: str) -> Optional[Any]:
        with self.lock:
            if key not in self.cache:
                return None
            # ç§»åˆ°æœ«å°¾ï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
            self.cache.move_to_end(key)
            return self.cache[key]
    
    def put(self, key: str, value: Any):
        with self.lock:
            if key in self.cache:
                # æ›´æ–°å¹¶ç§»åˆ°æœ«å°¾
                self.cache.move_to_end(key)
            self.cache[key] = value
            # è¶…è¿‡å®¹é‡ï¼Œåˆ é™¤æœ€æ—§çš„
            if len(self.cache) > self.capacity:
                self.cache.popitem(last=False)
    
    def remove(self, key: str):
        with self.lock:
            if key in self.cache:
                del self.cache[key]
    
    def clear(self):
        with self.lock:
            self.cache.clear()
    
    def size(self) -> int:
        with self.lock:
            return len(self.cache)


class AIDataCache:
    """AIå¯¹è¯æ•°æ®ç¼“å­˜ç®¡ç†å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, db_storage, config=None, max_cache_size: int = 10):
        self.db_storage = db_storage
        self.config = config
        # ä½¿ç”¨LRUç¼“å­˜æ›¿ä»£æ™®é€šå­—å…¸ï¼Œé™åˆ¶å†…å­˜ä½¿ç”¨
        self._cache = LRUCache(capacity=max_cache_size)
        self._timestamp_cache = {}  # ä»…å­˜å‚¨æ—¶é—´æˆ³ï¼Œè½»é‡çº§
        self._lock = Lock()
        
        # ç¼©çŸ­TTLï¼Œå‡å°‘è¿‡æœŸæ•°æ®é©»ç•™æ—¶é—´
        self.ttl = {
            'statistics': 180,      # 3åˆ†é’Ÿï¼ˆä»5åˆ†é’Ÿç¼©çŸ­ï¼‰
            'recent_samples': 120,  # 2åˆ†é’Ÿï¼ˆä»5åˆ†é’Ÿç¼©çŸ­ï¼‰
            'tag_analysis': 180,    # 3åˆ†é’Ÿï¼ˆä»5åˆ†é’Ÿç¼©çŸ­ï¼‰
        }
        
        logger.info(f"AIDataCache initialized with LRU (max_size={max_cache_size}, shorter TTL)")
    
    def _get_excluded_channel_ids(self) -> List[int]:
        """è·å–éœ€è¦æ’é™¤çš„é¢‘é“IDåˆ—è¡¨"""
        if not self.config:
            return []
        excluded = self.config.get('ai.exclude_from_context.channel_ids', [])
        return excluded if excluded else []
    
    def _get_excluded_tags(self) -> List[str]:
        """è·å–éœ€è¦æ’é™¤çš„æ ‡ç­¾åˆ—è¡¨"""
        if not self.config:
            return []
        excluded = self.config.get('ai.exclude_from_context.tags', [])
        return excluded if excluded else []
    
    def _should_apply_exclusion_to_interactions(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”å°†æ’é™¤è§„åˆ™åº”ç”¨äºAIäº’åŠ¨æ¨¡å¼çš„ç»Ÿè®¡ä¸åˆ†æ"""
        if not self.config:
            return True  # é»˜è®¤å¯ç”¨ä¿æŠ¤éšç§
        return self.config.get('ai.exclude_from_context.apply_to_ai_interactions', True)
    
    def get_statistics(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡æ•°æ®ï¼ˆæ€»æ•°ã€æ ‡ç­¾æ•°ã€æœ€è¿‘7å¤©ï¼‰"""
        return self._get_or_compute('statistics', self._compute_statistics)
    
    def get_recent_samples(self, limit: int = 10) -> list:
        """è·å–æœ€è¿‘å½’æ¡£ç¤ºä¾‹"""
        return self._get_or_compute('recent_samples', 
                                     lambda: self._compute_recent_samples(limit))
    
    def get_tag_analysis(self, limit: int = 15) -> list:
        """è·å–æ ‡ç­¾åˆ†æï¼ˆTOP Næ ‡ç­¾åŠè®¡æ•°ï¼‰"""
        return self._get_or_compute('tag_analysis',
                                     lambda: self._compute_tag_analysis(limit))
    
    def invalidate(self, *keys):
        """
        å¤±æ•ˆæŒ‡å®šç¼“å­˜
        
        Args:
            *keys: ç¼“å­˜é”®åï¼Œä¸ä¼ åˆ™æ¸…ç©ºå…¨éƒ¨
        """
        if not keys:
            self._cache.clear()
            with self._lock:
                self._timestamp_cache.clear()
            logger.debug("All cache cleared")
        else:
            for key in keys:
                self._cache.remove(key)
                with self._lock:
                    if key in self._timestamp_cache:
                        del self._timestamp_cache[key]
                logger.debug(f"Cache invalidated: {key}")
    
    def invalidate_all(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        self.invalidate()
    
    def _get_or_compute(self, key: str, compute_func):
        """é€šç”¨ç¼“å­˜è·å–æˆ–è®¡ç®—é€»è¾‘ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        # æ£€æŸ¥ç¼“å­˜å’Œæ—¶é—´æˆ³
        cached_data = self._cache.get(key)
        
        with self._lock:
            cached_time = self._timestamp_cache.get(key, 0)
        
        # ç¼“å­˜å‘½ä¸­ä¸”æœªè¿‡æœŸ
        if cached_data is not None and time.time() - cached_time < self.ttl.get(key, 180):
            logger.debug(f"Cache hit: {key}")
            return cached_data
        
        if cached_data is not None:
            logger.debug(f"Cache expired: {key}")
        
        # ç¼“å­˜æœªå‘½ä¸­æˆ–å·²è¿‡æœŸï¼Œé‡æ–°è®¡ç®—
        try:
            data = compute_func()
            
            # å­˜å…¥LRUç¼“å­˜
            self._cache.put(key, data)
            with self._lock:
                self._timestamp_cache[key] = time.time()
            
            logger.debug(f"Cache updated: {key}, size={self._cache.size()}")
            return data
        except Exception as e:
            logger.error(f"Cache computation error for {key}: {e}", exc_info=True)
            return self._get_fallback_data(key)
    
    def _compute_statistics(self) -> Dict[str, int]:
        """è®¡ç®—ç»Ÿè®¡æ•°æ®ï¼ˆå¯é€‰æ‹©æ˜¯å¦æ’é™¤æŒ‡å®šé¢‘é“å’Œæ ‡ç­¾ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åº”ç”¨æ’é™¤è§„åˆ™
        apply_exclusion = self._should_apply_exclusion_to_interactions()
        
        if not apply_exclusion:
            # ä¸åº”ç”¨æ’é™¤è§„åˆ™ï¼Œç»Ÿè®¡å…¨éƒ¨æ•°æ®
            total = self.db_storage.db.execute(
                "SELECT COUNT(*) FROM archives WHERE deleted = 0"
            ).fetchone()[0]
            
            tag_count = self.db_storage.db.execute("""
                SELECT COUNT(DISTINCT tag_id) FROM archive_tags 
                WHERE archive_id IN (SELECT id FROM archives WHERE deleted = 0)
            """).fetchone()[0]
            
            week_ago = int(time.time()) - 7 * 24 * 3600
            recent = self.db_storage.db.execute(
                "SELECT COUNT(*) FROM archives WHERE deleted = 0 AND created_at > ?",
                (week_ago,)
            ).fetchone()[0]
        else:
            # åº”ç”¨æ’é™¤è§„åˆ™
            excluded_channel_ids = self._get_excluded_channel_ids()
            excluded_tags = self._get_excluded_tags()
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_conditions = ["deleted = 0"]
            params = []
            
            # æ’é™¤æŒ‡å®šé¢‘é“çš„å†…å®¹
            if excluded_channel_ids:
                channel_conditions = []
                for channel_id in excluded_channel_ids:
                    channel_conditions.append("storage_path NOT LIKE ?")
                    params.append(f"telegram:{channel_id}:%")
                where_conditions.append(f"({' AND '.join(channel_conditions)})")
            
            # æ’é™¤åŒ…å«æŒ‡å®šæ ‡ç­¾çš„å½’æ¡£
            if excluded_tags:
                placeholders = ','.join(['?'] * len(excluded_tags))
                excluded_tag_ids_query = f"""
                    SELECT id FROM tags WHERE tag_name IN ({placeholders})
                """
                excluded_tag_ids = [
                    row[0] for row in self.db_storage.db.execute(
                        excluded_tag_ids_query, excluded_tags
                    ).fetchall()
                ]
                
                if excluded_tag_ids:
                    placeholders = ','.join(['?'] * len(excluded_tag_ids))
                    where_conditions.append(f"""
                        id NOT IN (
                            SELECT archive_id FROM archive_tags 
                            WHERE tag_id IN ({placeholders})
                        )
                    """)
                    params.extend(excluded_tag_ids)
            
            where_clause = " AND ".join(where_conditions)
            
            # è®¡ç®—æ€»æ•°
            total = self.db_storage.db.execute(
                f"SELECT COUNT(*) FROM archives WHERE {where_clause}",
                tuple(params)
            ).fetchone()[0]
            
            # è®¡ç®—æ ‡ç­¾æ•°ï¼ˆæ’é™¤æŒ‡å®šæ ‡ç­¾ï¼‰
            tag_where = ["archive_id IN (SELECT id FROM archives WHERE deleted = 0)"]
            tag_params = []
            
            if excluded_channel_ids:
                # æ’é™¤æ¥è‡ªæŒ‡å®šé¢‘é“çš„å½’æ¡£çš„æ ‡ç­¾
                channel_conditions = []
                for channel_id in excluded_channel_ids:
                    channel_conditions.append("(SELECT storage_path FROM archives WHERE id = archive_id) NOT LIKE ?")
                    tag_params.append(f"telegram:{channel_id}:%")
                tag_where.append(f"({' AND '.join(channel_conditions)})")
            
            if excluded_tags:
                placeholders = ','.join(['?'] * len(excluded_tags))
                tag_where.append(f"tag_id NOT IN (SELECT id FROM tags WHERE tag_name IN ({placeholders}))")
                tag_params.extend(excluded_tags)
            
            tag_where_clause = " AND ".join(tag_where)
            tag_count = self.db_storage.db.execute(
                f"SELECT COUNT(DISTINCT tag_id) FROM archive_tags WHERE {tag_where_clause}",
                tuple(tag_params)
            ).fetchone()[0]
            
            # è®¡ç®—æœ€è¿‘7å¤©
            week_ago = int(time.time()) - 7 * 24 * 3600
            recent_params = params + [week_ago]
            recent = self.db_storage.db.execute(
                f"SELECT COUNT(*) FROM archives WHERE {where_clause} AND created_at > ?",
                tuple(recent_params)
            ).fetchone()[0]
            
            if excluded_channel_ids or excluded_tags:
                logger.debug(f"Statistics filtered: excluded {len(excluded_channel_ids)} channels, {len(excluded_tags)} tags")
        
        stats = {
            'total': total,
            'tags': tag_count,
            'recent_week': recent
        }
        
        logger.debug(f"ğŸ“Š Statistics computed: {stats}")
        return stats
    
    def _compute_recent_samples(self, limit: int) -> list:
        """è®¡ç®—æœ€è¿‘å½’æ¡£ç¤ºä¾‹ï¼ˆæ’é™¤æŒ‡å®šé¢‘é“å’Œæ ‡ç­¾çš„å†…å®¹ï¼‰"""
        excluded_channel_ids = self._get_excluded_channel_ids()
        excluded_tags = self._get_excluded_tags()
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        where_conditions = ["deleted = 0"]
        params = []
        
        # æ’é™¤æŒ‡å®šé¢‘é“çš„å†…å®¹
        if excluded_channel_ids:
            # storage_path æ ¼å¼ä¸º "telegram:channel_id:message_id"
            channel_conditions = []
            for channel_id in excluded_channel_ids:
                channel_conditions.append("storage_path NOT LIKE ?")
                params.append(f"telegram:{channel_id}:%")
            where_conditions.append(f"({' AND '.join(channel_conditions)})")
        
        # æ’é™¤åŒ…å«æŒ‡å®šæ ‡ç­¾çš„å½’æ¡£
        if excluded_tags:
            # è·å–æ’é™¤æ ‡ç­¾çš„ID
            placeholders = ','.join(['?'] * len(excluded_tags))
            excluded_tag_ids_query = f"""
                SELECT id FROM tags WHERE tag_name IN ({placeholders})
            """
            excluded_tag_ids = [
                row[0] for row in self.db_storage.db.execute(
                    excluded_tag_ids_query, excluded_tags
                ).fetchall()
            ]
            
            if excluded_tag_ids:
                # æ’é™¤åŒ…å«è¿™äº›æ ‡ç­¾çš„å½’æ¡£
                placeholders = ','.join(['?'] * len(excluded_tag_ids))
                where_conditions.append(f"""
                    id NOT IN (
                        SELECT archive_id FROM archive_tags 
                        WHERE tag_id IN ({placeholders})
                    )
                """)
                params.extend(excluded_tag_ids)
        
        where_clause = " AND ".join(where_conditions)
        params.append(limit)
        
        query = f"""
            SELECT id, title, content, created_at FROM archives 
            WHERE {where_clause}
            ORDER BY created_at DESC 
            LIMIT ?
        """
        
        samples = self.db_storage.db.execute(query, tuple(params)).fetchall()
        
        result = [
            {
                'id': aid,
                'title': (title or content or 'æ— æ ‡é¢˜')[:50],
                'created_at': created_at
            }
            for aid, title, content, created_at in samples
        ]
        
        if excluded_channel_ids or excluded_tags:
            logger.debug(f"Recent samples filtered: excluded {len(excluded_channel_ids)} channels, {len(excluded_tags)} tags")
        
        logger.debug(f"ğŸ“ Recent samples: Retrieved {len(result)} items")
        
        return result
    
    def _compute_tag_analysis(self, limit: int) -> list:
        """è®¡ç®—æ ‡ç­¾åˆ†æï¼ˆæ’é™¤æŒ‡å®šæ ‡ç­¾ï¼‰"""
        excluded_tags = self._get_excluded_tags()
        excluded_channel_ids = self._get_excluded_channel_ids()
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        where_conditions = ["a.deleted = 0"]
        params = []
        
        # æ’é™¤æŒ‡å®šæ ‡ç­¾
        if excluded_tags:
            placeholders = ','.join(['?'] * len(excluded_tags))
            where_conditions.append(f"t.tag_name NOT IN ({placeholders})")
            params.extend(excluded_tags)
        
        # æ’é™¤æ¥è‡ªæŒ‡å®šé¢‘é“çš„å½’æ¡£
        if excluded_channel_ids:
            channel_conditions = []
            for channel_id in excluded_channel_ids:
                channel_conditions.append("a.storage_path NOT LIKE ?")
                params.append(f"telegram:{channel_id}:%")
            where_conditions.append(f"({' AND '.join(channel_conditions)})")
        
        where_clause = " AND ".join(where_conditions)
        params.append(limit)
        
        query = f"""
            SELECT t.tag_name, COUNT(*) as cnt
            FROM tags t
            JOIN archive_tags at ON t.id = at.tag_id
            JOIN archives a ON at.archive_id = a.id
            WHERE {where_clause}
            GROUP BY t.id
            ORDER BY cnt DESC
            LIMIT ?
        """
        
        tag_stats = self.db_storage.db.execute(query, tuple(params)).fetchall()
        
        result = [{'tag': tag, 'count': cnt} for tag, cnt in tag_stats]
        
        if excluded_tags or excluded_channel_ids:
            logger.debug(f"Tag analysis filtered: excluded {len(excluded_tags)} tags, {len(excluded_channel_ids)} channels")
        
        return result
    
    def _get_fallback_data(self, key: str):
        """è·å–å¤±è´¥æ—¶çš„å›é€€æ•°æ®"""
        fallbacks = {
            'statistics': {'total': 0, 'tags': 0, 'recent_week': 0},
            'recent_samples': [],
            'tag_analysis': []
        }
        return fallbacks.get(key, {})
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰"""
        info = {
            'cache_size': self._cache.size(),
            'max_capacity': self._cache.capacity,
            'items': {}
        }
        
        with self._lock:
            for key, timestamp in self._timestamp_cache.items():
                age = time.time() - timestamp
                info['items'][key] = {
                    'age_seconds': int(age),
                    'ttl': self.ttl.get(key, 180),
                    'expired': age >= self.ttl.get(key, 180)
                }
        
        return info
